{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unexpected-person",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cheap-advocate",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "champion-green",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>is_humor</th>\n",
       "      <th>humor_rating</th>\n",
       "      <th>humor_controversy</th>\n",
       "      <th>offense_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>TENNESSEE: We're the best state. Nobody even c...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A man inserted an advertisement in the classif...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>How many men does it take to open a can of bee...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Told my mom I hit 1200 Twitter followers. She ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Roses are dead. Love is fake. Weddings are bas...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  is_humor  \\\n",
       "0   1  TENNESSEE: We're the best state. Nobody even c...         1   \n",
       "1   2  A man inserted an advertisement in the classif...         1   \n",
       "2   3  How many men does it take to open a can of bee...         1   \n",
       "3   4  Told my mom I hit 1200 Twitter followers. She ...         1   \n",
       "4   5  Roses are dead. Love is fake. Weddings are bas...         1   \n",
       "\n",
       "   humor_rating  humor_controversy  offense_rating  \n",
       "0          2.42                1.0             0.2  \n",
       "1          2.50                1.0             1.1  \n",
       "2          1.95                0.0             2.4  \n",
       "3          2.11                1.0             0.0  \n",
       "4          2.78                0.0             0.1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "everyday-device",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "column = df[\"humor_rating\"]\n",
    "max_value = column.max()\n",
    "\n",
    "\n",
    "\n",
    "print(max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rolled-sessions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           senetence  label\n",
      "0  TENNESSEE: We're the best state. Nobody even c...      1\n",
      "1  A man inserted an advertisement in the classif...      1\n",
      "2  How many men does it take to open a can of bee...      1\n",
      "3  Told my mom I hit 1200 Twitter followers. She ...      1\n",
      "4  Roses are dead. Love is fake. Weddings are bas...      1\n",
      "5  'Trabajo,' the Spanish word for work, comes fr...      0\n",
      "6  I enrolled on some skill training and extra cu...      0\n",
      "7  ME: I'm such an original. Truly one of a kind....      1\n",
      "8  Men who ejaculated 21 times or more a month ha...      0\n",
      "9  I got REALLY angry today and it wasn't about n...      0\n"
     ]
    }
   ],
   "source": [
    "# Separate the json into sentences and labels\n",
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "#\n",
    "    \n",
    "for i,l,text,humour,rating,controversy,offense in df.itertuples():\n",
    "    sentences.append(text)\n",
    "    labels.append(humour)\n",
    "    \n",
    "print(pd.DataFrame({'senetence' : sentences[0:10], 'label':labels[0:10]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "incoming-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = round(len(sentences) * .75)\n",
    "\n",
    "training_sentences = sentences[0:training_size]\n",
    "testing_sentences = sentences[training_size:]\n",
    "training_labels = labels[0:training_size]\n",
    "testing_labels = labels[training_size:]\n",
    "\n",
    "\n",
    "# Setting tokenizer properties\n",
    "vocab_size = 20000\n",
    "oov_tok = \"<oov>\"\n",
    "\n",
    "# Fit the tokenizer on Training data\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Setting the padding properties\n",
    "max_length = 60\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "\n",
    "\n",
    "# Creating padded sequences from train and test data\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "young-hudson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DELL\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 60, 64)            1280000   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 24)                1560      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 1,281,585\n",
      "Trainable params: 1,281,585\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "embedding_dim = 64\n",
    "\n",
    "#embedding_dim = 16\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "occupied-timing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 2000 samples\n",
      "WARNING:tensorflow:From C:\\Users\\DELL\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/15\n",
      " - 24s - loss: 0.6374 - acc: 0.6307 - val_loss: 0.5632 - val_acc: 0.7240\n",
      "Epoch 2/15\n",
      " - 6s - loss: 0.4177 - acc: 0.8283 - val_loss: 0.3703 - val_acc: 0.8450\n",
      "Epoch 3/15\n",
      " - 6s - loss: 0.2601 - acc: 0.9030 - val_loss: 0.3317 - val_acc: 0.8615\n",
      "Epoch 4/15\n",
      " - 5s - loss: 0.1764 - acc: 0.9383 - val_loss: 0.3294 - val_acc: 0.8640\n",
      "Epoch 5/15\n",
      " - 5s - loss: 0.1226 - acc: 0.9642 - val_loss: 0.3368 - val_acc: 0.8670\n",
      "Epoch 6/15\n",
      " - 5s - loss: 0.0855 - acc: 0.9780 - val_loss: 0.3683 - val_acc: 0.8630\n",
      "Epoch 7/15\n",
      " - 5s - loss: 0.0609 - acc: 0.9857 - val_loss: 0.3871 - val_acc: 0.8640\n",
      "Epoch 8/15\n",
      " - 5s - loss: 0.0440 - acc: 0.9900 - val_loss: 0.4100 - val_acc: 0.8570\n",
      "Epoch 9/15\n",
      " - 6s - loss: 0.0327 - acc: 0.9943 - val_loss: 0.4319 - val_acc: 0.8540\n",
      "Epoch 10/15\n",
      " - 6s - loss: 0.0252 - acc: 0.9963 - val_loss: 0.4977 - val_acc: 0.8520\n",
      "Epoch 11/15\n",
      " - 6s - loss: 0.0192 - acc: 0.9970 - val_loss: 0.4853 - val_acc: 0.8460\n",
      "Epoch 12/15\n",
      " - 5s - loss: 0.0147 - acc: 0.9982 - val_loss: 0.5055 - val_acc: 0.8525\n",
      "Epoch 13/15\n",
      " - 6s - loss: 0.0117 - acc: 0.9987 - val_loss: 0.5290 - val_acc: 0.8420\n",
      "Epoch 14/15\n",
      " - 6s - loss: 0.0092 - acc: 0.9993 - val_loss: 0.5472 - val_acc: 0.8405\n",
      "Epoch 15/15\n",
      " - 6s - loss: 0.0075 - acc: 0.9993 - val_loss: 0.5653 - val_acc: 0.8385\n"
     ]
    }
   ],
   "source": [
    "# Converting the lists to numpy arrays for Tensorflow 2.x\n",
    "training_padded = np.array(training_padded)\n",
    "training_labels = np.array(training_labels)\n",
    "testing_padded = np.array(testing_padded)\n",
    "testing_labels = np.array(testing_labels)\n",
    "\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 15\n",
    "\n",
    "history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "brave-techno",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import scipy.stats\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) \n",
    "df['humor_controversy'] = df['humor_controversy'].replace(np.nan, 0)\n",
    "df['offense_rating'] = df['offense_rating'].replace(np.nan, 0)\n",
    "df['humor_rating'] = df['humor_rating'].replace(np.nan, 0)\n",
    "df['is_humor'] = df['is_humor'].replace(np.nan, 0)\n",
    "\n",
    "humor = df['is_humor'].values\n",
    "df1 = pd.DataFrame()\n",
    "#df['humor_rating'] = df['humor_rating'].replace(np.nan, 0)\n",
    "\n",
    "def Score_Predict2(df,model):\n",
    "    df['score']=df['text'].apply(lambda text: model.predict(pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=max_length, padding=padding_type, truncating=trunc_type)))\n",
    "    df['score'] = df['score'].replace(np.nan, 0)\n",
    "    df['score'] = df['score'].apply(lambda s: s[0][0])\n",
    "    df['My_rate']=df['score'].apply(lambda s: s*4)\n",
    "    df['new_rate']=df['My_rate'] - df['humor_controversy']-df['offense_rating']\n",
    "    df['new_rate'] = df['new_rate'].replace(np.nan, 0)\n",
    "    df['new_rate'] = df['new_rate']*df['is_humor']\n",
    "    actual = df['humor_rating'].values\n",
    "    pred = df['new_rate'].values\n",
    "    mae = metrics.mean_absolute_error(actual, pred)\n",
    "    print(\"MAE1:\",mae)\n",
    "    print(\"Pearson Corr.1 :\",scipy.stats.pearsonr(actual, pred)[0] )\n",
    "    print(\"Spearman corr.1 :\",scipy.stats.spearmanr(actual, pred)[0])\n",
    "    print(\"kendall.1 :\",scipy.stats.kendalltau(actual, pred)[0])\n",
    "    #2nd method\n",
    "    features = df[['humor_controversy','offense_rating','score','is_humor']].values\n",
    "    labels1 = df['humor_rating'].values\n",
    "    # getting training and testing data\n",
    "    features_train, features_test, labels_train1, labels_test1 = train_test_split(features, labels1, test_size = .75, random_state = 0)\n",
    "    scaler=MinMaxScaler()\n",
    "    scaler.fit(features_train)\n",
    "    features_train=scaler.transform(features_train)\n",
    "    features_test=scaler.transform(features_test)\n",
    "    model1=Sequential()\n",
    "    model1.add(Dense(4,activation='relu'))\n",
    "    model1.add(Dense(4,activation='relu'))\n",
    "    model1.add(Dense(4,activation='relu'))\n",
    "    model1.add(Dense(1))\n",
    "    model1.compile(loss='mse',optimizer='adam')#,metrics=['accuracy'])\n",
    "    history=model1.fit(x=features_train,y=labels_train1,epochs=30,validation_data=(features_test, labels_test1),verbose=2)\n",
    "    #humor_pred=model1.predict(features_test)\n",
    "    yhat = model1.predict(features, verbose=0)  \n",
    "    nice=[]\n",
    "    for i in yhat:\n",
    "        nice.append(i[0])\n",
    "    pred1 = [humor[i] * nice[i] for i in range(len(nice))]    \n",
    "    mae2 = metrics.mean_absolute_error(actual, pred1)\n",
    "    print(\"MAE2:\",mae2)\n",
    "    print(\"Pearson Corr.2 :\",scipy.stats.pearsonr(actual, pred1)[0] )\n",
    "    print(\"Spearman corr.2 :\",scipy.stats.spearmanr(actual, pred1)[0])\n",
    "    print(\"kendall.2 :\",scipy.stats.kendalltau(actual, pred1)[0])\n",
    "    d = {\"original\": actual, \"1st\": pred, \"2nd\": pred1}\n",
    "    df1 = pd.DataFrame(d)\n",
    "    print(df1 )\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "unsigned-brighton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE1: 0.6536443962625156\n",
      "Pearson Corr.1 : 0.7587482851006877\n",
      "Spearman corr.1 : 0.7331326649735217\n",
      "kendall.1 : 0.5517137776608328\n",
      "Train on 2000 samples, validate on 6000 samples\n",
      "Epoch 1/30\n",
      " - 3s - loss: 1.3027 - val_loss: 0.6628\n",
      "Epoch 2/30\n",
      " - 0s - loss: 0.5062 - val_loss: 0.3898\n",
      "Epoch 3/30\n",
      " - 0s - loss: 0.3979 - val_loss: 0.3369\n",
      "Epoch 4/30\n",
      " - 0s - loss: 0.3489 - val_loss: 0.2944\n",
      "Epoch 5/30\n",
      " - 0s - loss: 0.3100 - val_loss: 0.2620\n",
      "Epoch 6/30\n",
      " - 0s - loss: 0.2804 - val_loss: 0.2401\n",
      "Epoch 7/30\n",
      " - 0s - loss: 0.2594 - val_loss: 0.2232\n",
      "Epoch 8/30\n",
      " - 0s - loss: 0.2442 - val_loss: 0.2101\n",
      "Epoch 9/30\n",
      " - 0s - loss: 0.2312 - val_loss: 0.2013\n",
      "Epoch 10/30\n",
      " - 0s - loss: 0.2228 - val_loss: 0.1944\n",
      "Epoch 11/30\n",
      " - 0s - loss: 0.2149 - val_loss: 0.1911\n",
      "Epoch 12/30\n",
      " - 0s - loss: 0.2113 - val_loss: 0.1866\n",
      "Epoch 13/30\n",
      " - 0s - loss: 0.2059 - val_loss: 0.1835\n",
      "Epoch 14/30\n",
      " - 0s - loss: 0.2027 - val_loss: 0.1808\n",
      "Epoch 15/30\n",
      " - 0s - loss: 0.2008 - val_loss: 0.1793\n",
      "Epoch 16/30\n",
      " - 0s - loss: 0.1980 - val_loss: 0.1784\n",
      "Epoch 17/30\n",
      " - 0s - loss: 0.1976 - val_loss: 0.1774\n",
      "Epoch 18/30\n",
      " - 0s - loss: 0.1959 - val_loss: 0.1768\n",
      "Epoch 19/30\n",
      " - 0s - loss: 0.1948 - val_loss: 0.1766\n",
      "Epoch 20/30\n",
      " - 0s - loss: 0.1942 - val_loss: 0.1770\n",
      "Epoch 21/30\n",
      " - 0s - loss: 0.1931 - val_loss: 0.1762\n",
      "Epoch 22/30\n",
      " - 0s - loss: 0.1932 - val_loss: 0.1749\n",
      "Epoch 23/30\n",
      " - 0s - loss: 0.1923 - val_loss: 0.1746\n",
      "Epoch 24/30\n",
      " - 0s - loss: 0.1919 - val_loss: 0.1749\n",
      "Epoch 25/30\n",
      " - 0s - loss: 0.1906 - val_loss: 0.1768\n",
      "Epoch 26/30\n",
      " - 0s - loss: 0.1919 - val_loss: 0.1737\n",
      "Epoch 27/30\n",
      " - 0s - loss: 0.1909 - val_loss: 0.1751\n",
      "Epoch 28/30\n",
      " - 0s - loss: 0.1904 - val_loss: 0.1735\n",
      "Epoch 29/30\n",
      " - 0s - loss: 0.1899 - val_loss: 0.1728\n",
      "Epoch 30/30\n",
      " - 0s - loss: 0.1901 - val_loss: 0.1730\n",
      "MAE2: 0.38009544648289684\n",
      "Pearson Corr.2 : 0.8889834808882011\n",
      "Spearman corr.2 : 0.8293543454031753\n",
      "kendall.2 : 0.6507652909046695\n",
      "      original       1st       2nd\n",
      "0         2.42  2.782804  2.399918\n",
      "1         2.50  1.899095  1.699093\n",
      "2         1.95  1.599243  0.986068\n",
      "3         2.11  2.999549  2.559248\n",
      "4         2.78  3.894650  2.249848\n",
      "...        ...       ...       ...\n",
      "7995      0.00 -0.000000 -0.000000\n",
      "7996      1.33  0.097322  0.799465\n",
      "7997      2.55  0.693570  1.745895\n",
      "7998      1.00  0.999380  0.911924\n",
      "7999      0.00  0.000000 -0.000000\n",
      "\n",
      "[8000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "Score_Predict2(df,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "hybrid-longer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>is_humor</th>\n",
       "      <th>humor_rating</th>\n",
       "      <th>humor_controversy</th>\n",
       "      <th>offense_rating</th>\n",
       "      <th>score</th>\n",
       "      <th>My_rate</th>\n",
       "      <th>new_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>TENNESSEE: We're the best state. Nobody even c...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>9.957010e-01</td>\n",
       "      <td>3.982804e+00</td>\n",
       "      <td>2.782804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A man inserted an advertisement in the classif...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>9.997738e-01</td>\n",
       "      <td>3.999095e+00</td>\n",
       "      <td>1.899095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>How many men does it take to open a can of bee...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.40</td>\n",
       "      <td>9.998106e-01</td>\n",
       "      <td>3.999243e+00</td>\n",
       "      <td>1.599243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Told my mom I hit 1200 Twitter followers. She ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.998872e-01</td>\n",
       "      <td>3.999549e+00</td>\n",
       "      <td>2.999549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Roses are dead. Love is fake. Weddings are bas...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9.986625e-01</td>\n",
       "      <td>3.994650e+00</td>\n",
       "      <td>3.894650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>7996</td>\n",
       "      <td>Lack of awareness of the pervasiveness of raci...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.289564e-06</td>\n",
       "      <td>5.158257e-06</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>7997</td>\n",
       "      <td>Why are aspirins white? Because they work sorry</td>\n",
       "      <td>1</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>9.868304e-01</td>\n",
       "      <td>3.947322e+00</td>\n",
       "      <td>0.097322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>7998</td>\n",
       "      <td>Today, we Americans celebrate our independence...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.733924e-01</td>\n",
       "      <td>6.935695e-01</td>\n",
       "      <td>0.693570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>7999</td>\n",
       "      <td>How to keep the flies off the bride at an Ital...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>9.998449e-01</td>\n",
       "      <td>3.999380e+00</td>\n",
       "      <td>0.999380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>8000</td>\n",
       "      <td>\"Each ounce of sunflower seeds gives you 37% o...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.794844e-07</td>\n",
       "      <td>7.179376e-07</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  is_humor  \\\n",
       "0        1  TENNESSEE: We're the best state. Nobody even c...         1   \n",
       "1        2  A man inserted an advertisement in the classif...         1   \n",
       "2        3  How many men does it take to open a can of bee...         1   \n",
       "3        4  Told my mom I hit 1200 Twitter followers. She ...         1   \n",
       "4        5  Roses are dead. Love is fake. Weddings are bas...         1   \n",
       "...    ...                                                ...       ...   \n",
       "7995  7996  Lack of awareness of the pervasiveness of raci...         0   \n",
       "7996  7997    Why are aspirins white? Because they work sorry         1   \n",
       "7997  7998  Today, we Americans celebrate our independence...         1   \n",
       "7998  7999  How to keep the flies off the bride at an Ital...         1   \n",
       "7999  8000  \"Each ounce of sunflower seeds gives you 37% o...         0   \n",
       "\n",
       "      humor_rating  humor_controversy  offense_rating         score  \\\n",
       "0             2.42                1.0            0.20  9.957010e-01   \n",
       "1             2.50                1.0            1.10  9.997738e-01   \n",
       "2             1.95                0.0            2.40  9.998106e-01   \n",
       "3             2.11                1.0            0.00  9.998872e-01   \n",
       "4             2.78                0.0            0.10  9.986625e-01   \n",
       "...            ...                ...             ...           ...   \n",
       "7995          0.00                0.0            0.25  1.289564e-06   \n",
       "7996          1.33                0.0            3.85  9.868304e-01   \n",
       "7997          2.55                0.0            0.00  1.733924e-01   \n",
       "7998          1.00                0.0            3.00  9.998449e-01   \n",
       "7999          0.00                0.0            0.00  1.794844e-07   \n",
       "\n",
       "           My_rate  new_rate  \n",
       "0     3.982804e+00  2.782804  \n",
       "1     3.999095e+00  1.899095  \n",
       "2     3.999243e+00  1.599243  \n",
       "3     3.999549e+00  2.999549  \n",
       "4     3.994650e+00  3.894650  \n",
       "...            ...       ...  \n",
       "7995  5.158257e-06 -0.000000  \n",
       "7996  3.947322e+00  0.097322  \n",
       "7997  6.935695e-01  0.693570  \n",
       "7998  3.999380e+00  0.999380  \n",
       "7999  7.179376e-07  0.000000  \n",
       "\n",
       "[8000 rows x 9 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "combined-priority",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 60, 64)            1280000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_4 (Spatial (None, 60, 64)            0         \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 60, 32)            3104      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,283,649\n",
      "Trainable params: 1,283,649\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#RNN\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau \n",
    "\n",
    "model_rnn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.SpatialDropout1D(0.2),\n",
    "    tf.keras.layers.SimpleRNN(32, dropout=0.2, recurrent_dropout=0.2, return_sequences=True),\n",
    "    tf.keras.layers.GlobalMaxPooling1D(),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2)\n",
    "model_rnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "little-shape",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 2000 samples\n",
      "Epoch 1/15\n",
      "6000/6000 [==============================] - 12s 2ms/sample - loss: 0.6769 - acc: 0.5868 - val_loss: 0.6605 - val_acc: 0.6140\n",
      "Epoch 2/15\n",
      "6000/6000 [==============================] - 9s 2ms/sample - loss: 0.6554 - acc: 0.6185 - val_loss: 0.5741 - val_acc: 0.6200\n",
      "Epoch 3/15\n",
      "6000/6000 [==============================] - 9s 2ms/sample - loss: 0.5457 - acc: 0.7435 - val_loss: 0.4984 - val_acc: 0.7810\n",
      "Epoch 4/15\n",
      "6000/6000 [==============================] - 9s 1ms/sample - loss: 0.4712 - acc: 0.8003 - val_loss: 0.4840 - val_acc: 0.7760\n",
      "Epoch 5/15\n",
      "6000/6000 [==============================] - 8s 1ms/sample - loss: 0.4032 - acc: 0.8315 - val_loss: 0.4423 - val_acc: 0.8045\n",
      "Epoch 6/15\n",
      "6000/6000 [==============================] - 8s 1ms/sample - loss: 0.3316 - acc: 0.8748 - val_loss: 0.4080 - val_acc: 0.8265\n",
      "Epoch 7/15\n",
      "6000/6000 [==============================] - 8s 1ms/sample - loss: 0.2722 - acc: 0.8998 - val_loss: 0.4152 - val_acc: 0.8240\n",
      "Epoch 8/15\n",
      "6000/6000 [==============================] - 9s 1ms/sample - loss: 0.2154 - acc: 0.9255 - val_loss: 0.3775 - val_acc: 0.8505\n",
      "Epoch 9/15\n",
      "6000/6000 [==============================] - 8s 1ms/sample - loss: 0.1546 - acc: 0.9497 - val_loss: 0.4485 - val_acc: 0.8520\n",
      "Epoch 10/15\n",
      "6000/6000 [==============================] - 9s 1ms/sample - loss: 0.1116 - acc: 0.9653 - val_loss: 0.4267 - val_acc: 0.8475\n",
      "Epoch 11/15\n",
      "6000/6000 [==============================] - 9s 2ms/sample - loss: 0.0752 - acc: 0.9803 - val_loss: 0.4369 - val_acc: 0.8500\n",
      "Epoch 12/15\n",
      "6000/6000 [==============================] - 11s 2ms/sample - loss: 0.0748 - acc: 0.9808 - val_loss: 0.4462 - val_acc: 0.8495\n",
      "Epoch 13/15\n",
      "6000/6000 [==============================] - 9s 1ms/sample - loss: 0.0648 - acc: 0.9810 - val_loss: 0.4474 - val_acc: 0.8490\n",
      "Epoch 14/15\n",
      "6000/6000 [==============================] - 8s 1ms/sample - loss: 0.0684 - acc: 0.9802 - val_loss: 0.4490 - val_acc: 0.8480\n",
      "Epoch 15/15\n",
      "6000/6000 [==============================] - 8s 1ms/sample - loss: 0.0669 - acc: 0.9823 - val_loss: 0.4491 - val_acc: 0.8485\n"
     ]
    }
   ],
   "source": [
    "history_rnn = model_rnn.fit(training_padded, training_labels, batch_size=32, epochs=15, \n",
    "                    validation_data=(testing_padded, testing_labels), \n",
    "                    callbacks=[rlrp] ,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dirty-semester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE1: 0.6543699146240624\n",
      "Pearson Corr.1 : 0.7523786622062567\n",
      "Spearman corr.1 : 0.7281117880672627\n",
      "kendall.1 : 0.5491269990936982\n",
      "Train on 2000 samples, validate on 6000 samples\n",
      "Epoch 1/30\n",
      " - 2s - loss: 3.2246 - val_loss: 2.9667\n",
      "Epoch 2/30\n",
      " - 0s - loss: 2.7651 - val_loss: 2.4296\n",
      "Epoch 3/30\n",
      " - 0s - loss: 2.1226 - val_loss: 1.6918\n",
      "Epoch 4/30\n",
      " - 0s - loss: 1.3196 - val_loss: 0.8795\n",
      "Epoch 5/30\n",
      " - 0s - loss: 0.6118 - val_loss: 0.3528\n",
      "Epoch 6/30\n",
      " - 0s - loss: 0.2836 - val_loss: 0.2098\n",
      "Epoch 7/30\n",
      " - 0s - loss: 0.2119 - val_loss: 0.1844\n",
      "Epoch 8/30\n",
      " - 0s - loss: 0.1958 - val_loss: 0.1769\n",
      "Epoch 9/30\n",
      " - 0s - loss: 0.1905 - val_loss: 0.1748\n",
      "Epoch 10/30\n",
      " - 0s - loss: 0.1885 - val_loss: 0.1729\n",
      "Epoch 11/30\n",
      " - 0s - loss: 0.1873 - val_loss: 0.1735\n",
      "Epoch 12/30\n",
      " - 0s - loss: 0.1868 - val_loss: 0.1719\n",
      "Epoch 13/30\n",
      " - 0s - loss: 0.1876 - val_loss: 0.1720\n",
      "Epoch 14/30\n",
      " - 0s - loss: 0.1859 - val_loss: 0.1722\n",
      "Epoch 15/30\n",
      " - 0s - loss: 0.1847 - val_loss: 0.1771\n",
      "Epoch 16/30\n",
      " - 0s - loss: 0.1868 - val_loss: 0.1722\n",
      "Epoch 17/30\n",
      " - 0s - loss: 0.1856 - val_loss: 0.1714\n",
      "Epoch 18/30\n",
      " - 0s - loss: 0.1850 - val_loss: 0.1710\n",
      "Epoch 19/30\n",
      " - 0s - loss: 0.1850 - val_loss: 0.1710\n",
      "Epoch 20/30\n",
      " - 0s - loss: 0.1850 - val_loss: 0.1720\n",
      "Epoch 21/30\n",
      " - 0s - loss: 0.1860 - val_loss: 0.1705\n",
      "Epoch 22/30\n",
      " - 0s - loss: 0.1853 - val_loss: 0.1716\n",
      "Epoch 23/30\n",
      " - 0s - loss: 0.1850 - val_loss: 0.1716\n",
      "Epoch 24/30\n",
      " - 0s - loss: 0.1837 - val_loss: 0.1710\n",
      "Epoch 25/30\n",
      " - 0s - loss: 0.1847 - val_loss: 0.1714\n",
      "Epoch 26/30\n",
      " - 0s - loss: 0.1845 - val_loss: 0.1710\n",
      "Epoch 27/30\n",
      " - 0s - loss: 0.1843 - val_loss: 0.1703\n",
      "Epoch 28/30\n",
      " - 0s - loss: 0.1844 - val_loss: 0.1705\n",
      "Epoch 29/30\n",
      " - 0s - loss: 0.1845 - val_loss: 0.1704\n",
      "Epoch 30/30\n",
      " - 0s - loss: 0.1842 - val_loss: 0.1706\n",
      "MAE2: 0.4780598907309771\n",
      "Pearson Corr.2 : 0.8056644110401607\n",
      "Spearman corr.2 : 0.7667875605201543\n",
      "kendall.2 : 0.5779298028378338\n",
      "      original       1st       2nd\n",
      "0         2.42  2.477636  2.443468\n",
      "1         2.50  1.896951  1.709341\n",
      "2         1.95  1.583549  0.493178\n",
      "3         2.11  2.998111  2.510844\n",
      "4         2.78  3.845691  2.241310\n",
      "...        ...       ...       ...\n",
      "7995      0.00 -0.000000 -0.000000\n",
      "7996      1.33  0.130004 -0.557655\n",
      "7997      2.55  0.071444  1.592695\n",
      "7998      1.00  0.998852  0.058250\n",
      "7999      0.00  0.000000  0.000000\n",
      "\n",
      "[8000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "Score_Predict2(df,model_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "steady-synthetic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>is_humor</th>\n",
       "      <th>humor_rating</th>\n",
       "      <th>humor_controversy</th>\n",
       "      <th>offense_rating</th>\n",
       "      <th>score</th>\n",
       "      <th>My_rate</th>\n",
       "      <th>new_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>TENNESSEE: We're the best state. Nobody even c...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.919409</td>\n",
       "      <td>3.677636</td>\n",
       "      <td>2.477636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A man inserted an advertisement in the classif...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.999238</td>\n",
       "      <td>3.996951</td>\n",
       "      <td>1.896951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>How many men does it take to open a can of bee...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.995887</td>\n",
       "      <td>3.983549</td>\n",
       "      <td>1.583549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Told my mom I hit 1200 Twitter followers. She ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.999528</td>\n",
       "      <td>3.998111</td>\n",
       "      <td>2.998111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Roses are dead. Love is fake. Weddings are bas...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.986423</td>\n",
       "      <td>3.945691</td>\n",
       "      <td>3.845691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>7996</td>\n",
       "      <td>Lack of awareness of the pervasiveness of raci...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.002014</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>7997</td>\n",
       "      <td>Why are aspirins white? Because they work sorry</td>\n",
       "      <td>1</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>0.995001</td>\n",
       "      <td>3.980004</td>\n",
       "      <td>0.130004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>7998</td>\n",
       "      <td>Today, we Americans celebrate our independence...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.017861</td>\n",
       "      <td>0.071444</td>\n",
       "      <td>0.071444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>7999</td>\n",
       "      <td>How to keep the flies off the bride at an Ital...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.999713</td>\n",
       "      <td>3.998852</td>\n",
       "      <td>0.998852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>8000</td>\n",
       "      <td>\"Each ounce of sunflower seeds gives you 37% o...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.002207</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  is_humor  \\\n",
       "0        1  TENNESSEE: We're the best state. Nobody even c...         1   \n",
       "1        2  A man inserted an advertisement in the classif...         1   \n",
       "2        3  How many men does it take to open a can of bee...         1   \n",
       "3        4  Told my mom I hit 1200 Twitter followers. She ...         1   \n",
       "4        5  Roses are dead. Love is fake. Weddings are bas...         1   \n",
       "...    ...                                                ...       ...   \n",
       "7995  7996  Lack of awareness of the pervasiveness of raci...         0   \n",
       "7996  7997    Why are aspirins white? Because they work sorry         1   \n",
       "7997  7998  Today, we Americans celebrate our independence...         1   \n",
       "7998  7999  How to keep the flies off the bride at an Ital...         1   \n",
       "7999  8000  \"Each ounce of sunflower seeds gives you 37% o...         0   \n",
       "\n",
       "      humor_rating  humor_controversy  offense_rating     score   My_rate  \\\n",
       "0             2.42                1.0            0.20  0.919409  3.677636   \n",
       "1             2.50                1.0            1.10  0.999238  3.996951   \n",
       "2             1.95                0.0            2.40  0.995887  3.983549   \n",
       "3             2.11                1.0            0.00  0.999528  3.998111   \n",
       "4             2.78                0.0            0.10  0.986423  3.945691   \n",
       "...            ...                ...             ...       ...       ...   \n",
       "7995          0.00                0.0            0.25  0.000504  0.002014   \n",
       "7996          1.33                0.0            3.85  0.995001  3.980004   \n",
       "7997          2.55                0.0            0.00  0.017861  0.071444   \n",
       "7998          1.00                0.0            3.00  0.999713  3.998852   \n",
       "7999          0.00                0.0            0.00  0.000552  0.002207   \n",
       "\n",
       "      new_rate  \n",
       "0     2.477636  \n",
       "1     1.896951  \n",
       "2     1.583549  \n",
       "3     2.998111  \n",
       "4     3.845691  \n",
       "...        ...  \n",
       "7995 -0.000000  \n",
       "7996  0.130004  \n",
       "7997  0.071444  \n",
       "7998  0.998852  \n",
       "7999  0.000000  \n",
       "\n",
       "[8000 rows x 9 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ordered-designer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 60, 64)            1280000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_5 (Spatial (None, 60, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 60, 32)            12416     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,292,961\n",
      "Trainable params: 1,292,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_lstm_avg = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.SpatialDropout1D(0.2),\n",
    "    tf.keras.layers.LSTM(32, dropout=0.2, recurrent_dropout=0.2, return_sequences=True),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2)\n",
    "model_lstm_avg.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_lstm_avg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "superior-salem",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 2000 samples\n",
      "Epoch 1/15\n",
      "6000/6000 [==============================] - 23s 4ms/sample - loss: 0.5737 - acc: 0.6975 - val_loss: 0.4270 - val_acc: 0.8160\n",
      "Epoch 2/15\n",
      "6000/6000 [==============================] - 18s 3ms/sample - loss: 0.3644 - acc: 0.8623 - val_loss: 0.3580 - val_acc: 0.8560\n",
      "Epoch 3/15\n",
      "6000/6000 [==============================] - 18s 3ms/sample - loss: 0.2353 - acc: 0.9243 - val_loss: 0.4175 - val_acc: 0.8545\n",
      "Epoch 4/15\n",
      "6000/6000 [==============================] - 19s 3ms/sample - loss: 0.1602 - acc: 0.9538 - val_loss: 0.4328 - val_acc: 0.8530\n",
      "Epoch 5/15\n",
      "6000/6000 [==============================] - 18s 3ms/sample - loss: 0.1072 - acc: 0.9738 - val_loss: 0.4387 - val_acc: 0.8550\n",
      "Epoch 6/15\n",
      "6000/6000 [==============================] - 17s 3ms/sample - loss: 0.0943 - acc: 0.9792 - val_loss: 0.4728 - val_acc: 0.8550\n",
      "Epoch 7/15\n",
      "6000/6000 [==============================] - 18s 3ms/sample - loss: 0.0916 - acc: 0.9778 - val_loss: 0.4727 - val_acc: 0.8555\n",
      "Epoch 8/15\n",
      "6000/6000 [==============================] - 17s 3ms/sample - loss: 0.0938 - acc: 0.9783 - val_loss: 0.4727 - val_acc: 0.8555\n",
      "Epoch 9/15\n",
      "6000/6000 [==============================] - 17s 3ms/sample - loss: 0.0871 - acc: 0.9790 - val_loss: 0.4725 - val_acc: 0.8555\n",
      "Epoch 10/15\n",
      "6000/6000 [==============================] - 18s 3ms/sample - loss: 0.0868 - acc: 0.9807 - val_loss: 0.4727 - val_acc: 0.8555\n",
      "Epoch 11/15\n",
      "6000/6000 [==============================] - 20s 3ms/sample - loss: 0.0911 - acc: 0.9780 - val_loss: 0.4727 - val_acc: 0.8555\n",
      "Epoch 12/15\n",
      "6000/6000 [==============================] - 17s 3ms/sample - loss: 0.0869 - acc: 0.9792 - val_loss: 0.4727 - val_acc: 0.8555\n",
      "Epoch 13/15\n",
      "6000/6000 [==============================] - 17s 3ms/sample - loss: 0.0931 - acc: 0.9775 - val_loss: 0.4727 - val_acc: 0.8555loss: 0.0916 - acc: 0.\n",
      "Epoch 14/15\n",
      "6000/6000 [==============================] - 17s 3ms/sample - loss: 0.0875 - acc: 0.9795 - val_loss: 0.4727 - val_acc: 0.8555 0.97\n",
      "Epoch 15/15\n",
      "6000/6000 [==============================] - 17s 3ms/sample - loss: 0.0889 - acc: 0.9787 - val_loss: 0.4727 - val_acc: 0.8555\n"
     ]
    }
   ],
   "source": [
    "history_lstm_avg = model_lstm_avg.fit(training_padded, training_labels, batch_size=32, epochs=num_epochs, \n",
    "                    validation_data=(testing_padded, testing_labels), \n",
    "                    callbacks=[rlrp] ,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "interesting-canon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE1: 0.6552756379737426\n",
      "Pearson Corr.1 : 0.7487524373766052\n",
      "Spearman corr.1 : 0.7241805223880627\n",
      "kendall.1 : 0.5471831294158913\n",
      "Train on 2000 samples, validate on 6000 samples\n",
      "Epoch 1/30\n",
      " - 3s - loss: 2.9971 - val_loss: 2.4687\n",
      "Epoch 2/30\n",
      " - 0s - loss: 1.9633 - val_loss: 1.3316\n",
      "Epoch 3/30\n",
      " - 0s - loss: 0.9895 - val_loss: 0.6680\n",
      "Epoch 4/30\n",
      " - 0s - loss: 0.5220 - val_loss: 0.3798\n",
      "Epoch 5/30\n",
      " - 0s - loss: 0.3308 - val_loss: 0.2685\n",
      "Epoch 6/30\n",
      " - 0s - loss: 0.2589 - val_loss: 0.2265\n",
      "Epoch 7/30\n",
      " - 0s - loss: 0.2307 - val_loss: 0.2073\n",
      "Epoch 8/30\n",
      " - 0s - loss: 0.2159 - val_loss: 0.1982\n",
      "Epoch 9/30\n",
      " - 0s - loss: 0.2085 - val_loss: 0.1939\n",
      "Epoch 10/30\n",
      " - 0s - loss: 0.2038 - val_loss: 0.1888\n",
      "Epoch 11/30\n",
      " - 0s - loss: 0.2014 - val_loss: 0.1847\n",
      "Epoch 12/30\n",
      " - 0s - loss: 0.2002 - val_loss: 0.1826\n",
      "Epoch 13/30\n",
      " - 0s - loss: 0.1980 - val_loss: 0.1817\n",
      "Epoch 14/30\n",
      " - 0s - loss: 0.1965 - val_loss: 0.1811\n",
      "Epoch 15/30\n",
      " - 0s - loss: 0.1950 - val_loss: 0.1788\n",
      "Epoch 16/30\n",
      " - 0s - loss: 0.1940 - val_loss: 0.1783\n",
      "Epoch 17/30\n",
      " - 0s - loss: 0.1930 - val_loss: 0.1765\n",
      "Epoch 18/30\n",
      " - 0s - loss: 0.1918 - val_loss: 0.1765\n",
      "Epoch 19/30\n",
      " - 0s - loss: 0.1925 - val_loss: 0.1745\n",
      "Epoch 20/30\n",
      " - 0s - loss: 0.1907 - val_loss: 0.1737\n",
      "Epoch 21/30\n",
      " - 0s - loss: 0.1903 - val_loss: 0.1732\n",
      "Epoch 22/30\n",
      " - 0s - loss: 0.1887 - val_loss: 0.1730\n",
      "Epoch 23/30\n",
      " - 0s - loss: 0.1881 - val_loss: 0.1724\n",
      "Epoch 24/30\n",
      " - 0s - loss: 0.1873 - val_loss: 0.1724\n",
      "Epoch 25/30\n",
      " - 0s - loss: 0.1879 - val_loss: 0.1726\n",
      "Epoch 26/30\n",
      " - 0s - loss: 0.1874 - val_loss: 0.1735\n",
      "Epoch 27/30\n",
      " - 0s - loss: 0.1875 - val_loss: 0.1715\n",
      "Epoch 28/30\n",
      " - 0s - loss: 0.1872 - val_loss: 0.1712\n",
      "Epoch 29/30\n",
      " - 0s - loss: 0.1864 - val_loss: 0.1712\n",
      "Epoch 30/30\n",
      " - 0s - loss: 0.1871 - val_loss: 0.1712\n",
      "MAE2: 0.46144739620035513\n",
      "Pearson Corr.2 : 0.8143180683778508\n",
      "Spearman corr.2 : 0.7702956694979726\n",
      "kendall.2 : 0.5793623999776087\n",
      "      original       1st       2nd\n",
      "0         2.42  2.678530  2.390364\n",
      "1         2.50  1.863352  2.328399\n",
      "2         1.95  1.541121  0.016910\n",
      "3         2.11  2.975257  2.407659\n",
      "4         2.78  3.846382  2.262139\n",
      "...        ...       ...       ...\n",
      "7995      0.00 -0.000000  0.000000\n",
      "7996      1.33  0.121397 -0.067438\n",
      "7997      2.55  0.060814  2.239178\n",
      "7998      1.00  0.962796 -0.017908\n",
      "7999      0.00  0.000000  0.000000\n",
      "\n",
      "[8000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "Score_Predict2(df,model_lstm_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "legendary-volleyball",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>is_humor</th>\n",
       "      <th>humor_rating</th>\n",
       "      <th>humor_controversy</th>\n",
       "      <th>offense_rating</th>\n",
       "      <th>score</th>\n",
       "      <th>My_rate</th>\n",
       "      <th>new_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>TENNESSEE: We're the best state. Nobody even c...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.969633</td>\n",
       "      <td>3.878530</td>\n",
       "      <td>2.678530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A man inserted an advertisement in the classif...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.990838</td>\n",
       "      <td>3.963352</td>\n",
       "      <td>1.863352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>How many men does it take to open a can of bee...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.985280</td>\n",
       "      <td>3.941121</td>\n",
       "      <td>1.541121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Told my mom I hit 1200 Twitter followers. She ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.993814</td>\n",
       "      <td>3.975257</td>\n",
       "      <td>2.975257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Roses are dead. Love is fake. Weddings are bas...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.986595</td>\n",
       "      <td>3.946382</td>\n",
       "      <td>3.846382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>7996</td>\n",
       "      <td>Lack of awareness of the pervasiveness of raci...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.009065</td>\n",
       "      <td>0.036260</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>7997</td>\n",
       "      <td>Why are aspirins white? Because they work sorry</td>\n",
       "      <td>1</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>0.992849</td>\n",
       "      <td>3.971397</td>\n",
       "      <td>0.121397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>7998</td>\n",
       "      <td>Today, we Americans celebrate our independence...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.015204</td>\n",
       "      <td>0.060814</td>\n",
       "      <td>0.060814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>7999</td>\n",
       "      <td>How to keep the flies off the bride at an Ital...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.990699</td>\n",
       "      <td>3.962796</td>\n",
       "      <td>0.962796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>8000</td>\n",
       "      <td>\"Each ounce of sunflower seeds gives you 37% o...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.008222</td>\n",
       "      <td>0.032887</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  is_humor  \\\n",
       "0        1  TENNESSEE: We're the best state. Nobody even c...         1   \n",
       "1        2  A man inserted an advertisement in the classif...         1   \n",
       "2        3  How many men does it take to open a can of bee...         1   \n",
       "3        4  Told my mom I hit 1200 Twitter followers. She ...         1   \n",
       "4        5  Roses are dead. Love is fake. Weddings are bas...         1   \n",
       "...    ...                                                ...       ...   \n",
       "7995  7996  Lack of awareness of the pervasiveness of raci...         0   \n",
       "7996  7997    Why are aspirins white? Because they work sorry         1   \n",
       "7997  7998  Today, we Americans celebrate our independence...         1   \n",
       "7998  7999  How to keep the flies off the bride at an Ital...         1   \n",
       "7999  8000  \"Each ounce of sunflower seeds gives you 37% o...         0   \n",
       "\n",
       "      humor_rating  humor_controversy  offense_rating     score   My_rate  \\\n",
       "0             2.42                1.0            0.20  0.969633  3.878530   \n",
       "1             2.50                1.0            1.10  0.990838  3.963352   \n",
       "2             1.95                0.0            2.40  0.985280  3.941121   \n",
       "3             2.11                1.0            0.00  0.993814  3.975257   \n",
       "4             2.78                0.0            0.10  0.986595  3.946382   \n",
       "...            ...                ...             ...       ...       ...   \n",
       "7995          0.00                0.0            0.25  0.009065  0.036260   \n",
       "7996          1.33                0.0            3.85  0.992849  3.971397   \n",
       "7997          2.55                0.0            0.00  0.015204  0.060814   \n",
       "7998          1.00                0.0            3.00  0.990699  3.962796   \n",
       "7999          0.00                0.0            0.00  0.008222  0.032887   \n",
       "\n",
       "      new_rate  \n",
       "0     2.678530  \n",
       "1     1.863352  \n",
       "2     1.541121  \n",
       "3     2.975257  \n",
       "4     3.846382  \n",
       "...        ...  \n",
       "7995 -0.000000  \n",
       "7996  0.121397  \n",
       "7997  0.060814  \n",
       "7998  0.962796  \n",
       "7999  0.000000  \n",
       "\n",
       "[8000 rows x 9 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "separated-creativity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 60, 64)            1280000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_6 (Spatial (None, 60, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 60, 32)            12416     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,292,449\n",
      "Trainable params: 1,292,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau \n",
    "\n",
    "model_lstm1 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.SpatialDropout1D(0.2),\n",
    "    tf.keras.layers.LSTM(32, dropout=0.2, recurrent_dropout=0.2, return_sequences=True),\n",
    "    tf.keras.layers.GlobalMaxPooling1D(),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2)\n",
    "model_lstm1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_lstm1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "flying-reason",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 2000 samples\n",
      "Epoch 1/15\n",
      "6000/6000 [==============================] - 20s 3ms/sample - loss: 0.5695 - acc: 0.6920 - val_loss: 0.4079 - val_acc: 0.8195\n",
      "Epoch 2/15\n",
      "6000/6000 [==============================] - 17s 3ms/sample - loss: 0.3170 - acc: 0.8722 - val_loss: 0.3394 - val_acc: 0.8540\n",
      "Epoch 3/15\n",
      "6000/6000 [==============================] - 17s 3ms/sample - loss: 0.1881 - acc: 0.9347 - val_loss: 0.3706 - val_acc: 0.8550\n",
      "Epoch 4/15\n",
      "6000/6000 [==============================] - 18s 3ms/sample - loss: 0.1088 - acc: 0.9658 - val_loss: 0.4099 - val_acc: 0.8415\n",
      "Epoch 5/15\n",
      "6000/6000 [==============================] - 17s 3ms/sample - loss: 0.0584 - acc: 0.9835 - val_loss: 0.4055 - val_acc: 0.8445\n",
      "Epoch 6/15\n",
      "6000/6000 [==============================] - 17s 3ms/sample - loss: 0.0506 - acc: 0.9863 - val_loss: 0.4361 - val_acc: 0.8460\n",
      "Epoch 7/15\n",
      "6000/6000 [==============================] - 17s 3ms/sample - loss: 0.0481 - acc: 0.9868 - val_loss: 0.4376 - val_acc: 0.8465\n",
      "Epoch 8/15\n",
      "6000/6000 [==============================] - 17s 3ms/sample - loss: 0.0450 - acc: 0.9882 - val_loss: 0.4404 - val_acc: 0.8460\n",
      "Epoch 9/15\n",
      "6000/6000 [==============================] - 17s 3ms/sample - loss: 0.0459 - acc: 0.9880 - val_loss: 0.4406 - val_acc: 0.8460\n",
      "Epoch 10/15\n",
      "6000/6000 [==============================] - 17s 3ms/sample - loss: 0.0487 - acc: 0.9870 - val_loss: 0.4408 - val_acc: 0.8460\n",
      "Epoch 11/15\n",
      "6000/6000 [==============================] - 17s 3ms/sample - loss: 0.0463 - acc: 0.9883 - val_loss: 0.4408 - val_acc: 0.8465\n",
      "Epoch 12/15\n",
      "6000/6000 [==============================] - 17s 3ms/sample - loss: 0.0477 - acc: 0.9870 - val_loss: 0.4408 - val_acc: 0.8460\n",
      "Epoch 13/15\n",
      "6000/6000 [==============================] - 17s 3ms/sample - loss: 0.0490 - acc: 0.9878 - val_loss: 0.4408 - val_acc: 0.8460\n",
      "Epoch 14/15\n",
      "6000/6000 [==============================] - 17s 3ms/sample - loss: 0.0485 - acc: 0.9863 - val_loss: 0.4408 - val_acc: 0.8465\n",
      "Epoch 15/15\n",
      "6000/6000 [==============================] - 17s 3ms/sample - loss: 0.0461 - acc: 0.9883 - val_loss: 0.4408 - val_acc: 0.8465\n"
     ]
    }
   ],
   "source": [
    "history_lstm1 = model_lstm1.fit(training_padded, training_labels, batch_size=32, epochs=num_epochs, \n",
    "                    validation_data=(testing_padded, testing_labels), \n",
    "                    callbacks=[rlrp] ,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "accredited-armenia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE1: 0.6541895267407176\n",
      "Pearson Corr.1 : 0.7480504976252162\n",
      "Spearman corr.1 : 0.7250628800064474\n",
      "kendall.1 : 0.5473857281360217\n",
      "Train on 2000 samples, validate on 6000 samples\n",
      "Epoch 1/30\n",
      " - 3s - loss: 3.2443 - val_loss: 2.6736\n",
      "Epoch 2/30\n",
      " - 0s - loss: 2.2390 - val_loss: 1.7790\n",
      "Epoch 3/30\n",
      " - 0s - loss: 1.5147 - val_loss: 1.2383\n",
      "Epoch 4/30\n",
      " - 0s - loss: 1.0919 - val_loss: 0.8935\n",
      "Epoch 5/30\n",
      " - 0s - loss: 0.7673 - val_loss: 0.5918\n",
      "Epoch 6/30\n",
      " - 0s - loss: 0.5266 - val_loss: 0.4193\n",
      "Epoch 7/30\n",
      " - 0s - loss: 0.4025 - val_loss: 0.3304\n",
      "Epoch 8/30\n",
      " - 0s - loss: 0.3308 - val_loss: 0.2789\n",
      "Epoch 9/30\n",
      " - 0s - loss: 0.2906 - val_loss: 0.2505\n",
      "Epoch 10/30\n",
      " - 0s - loss: 0.2661 - val_loss: 0.2314\n",
      "Epoch 11/30\n",
      " - 0s - loss: 0.2484 - val_loss: 0.2176\n",
      "Epoch 12/30\n",
      " - 0s - loss: 0.2336 - val_loss: 0.2034\n",
      "Epoch 13/30\n",
      " - 0s - loss: 0.2184 - val_loss: 0.1916\n",
      "Epoch 14/30\n",
      " - 0s - loss: 0.2055 - val_loss: 0.1813\n",
      "Epoch 15/30\n",
      " - 0s - loss: 0.1949 - val_loss: 0.1747\n",
      "Epoch 16/30\n",
      " - 0s - loss: 0.1889 - val_loss: 0.1721\n",
      "Epoch 17/30\n",
      " - 0s - loss: 0.1866 - val_loss: 0.1717\n",
      "Epoch 18/30\n",
      " - 0s - loss: 0.1860 - val_loss: 0.1715\n",
      "Epoch 19/30\n",
      " - 0s - loss: 0.1855 - val_loss: 0.1707\n",
      "Epoch 20/30\n",
      " - 0s - loss: 0.1855 - val_loss: 0.1705\n",
      "Epoch 21/30\n",
      " - 0s - loss: 0.1843 - val_loss: 0.1698\n",
      "Epoch 22/30\n",
      " - 0s - loss: 0.1833 - val_loss: 0.1733\n",
      "Epoch 23/30\n",
      " - 0s - loss: 0.1830 - val_loss: 0.1696\n",
      "Epoch 24/30\n",
      " - 0s - loss: 0.1832 - val_loss: 0.1690\n",
      "Epoch 25/30\n",
      " - 0s - loss: 0.1831 - val_loss: 0.1689\n",
      "Epoch 26/30\n",
      " - 0s - loss: 0.1825 - val_loss: 0.1689\n",
      "Epoch 27/30\n",
      " - 0s - loss: 0.1823 - val_loss: 0.1685\n",
      "Epoch 28/30\n",
      " - 0s - loss: 0.1822 - val_loss: 0.1685\n",
      "Epoch 29/30\n",
      " - 0s - loss: 0.1819 - val_loss: 0.1682\n",
      "Epoch 30/30\n",
      " - 0s - loss: 0.1817 - val_loss: 0.1681\n",
      "MAE2: 0.48214418437043205\n",
      "Pearson Corr.2 : 0.8077645707433804\n",
      "Spearman corr.2 : 0.8284212318297532\n",
      "kendall.2 : 0.6500727246673157\n",
      "      original       1st       2nd\n",
      "0         2.42  2.013413  2.422941\n",
      "1         2.50  1.885436  1.730727\n",
      "2         1.95  1.579802  0.142508\n",
      "3         2.11  2.993006  2.449691\n",
      "4         2.78  3.863475  2.245118\n",
      "...        ...       ...       ...\n",
      "7995      0.00 -0.000000  0.000000\n",
      "7996      1.33  0.124817  0.030072\n",
      "7997      2.55  0.132206  1.879524\n",
      "7998      1.00  0.978916  0.030072\n",
      "7999      0.00  0.000000  0.000000\n",
      "\n",
      "[8000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "Score_Predict2(df,model_lstm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "trying-defensive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>is_humor</th>\n",
       "      <th>humor_rating</th>\n",
       "      <th>humor_controversy</th>\n",
       "      <th>offense_rating</th>\n",
       "      <th>score</th>\n",
       "      <th>My_rate</th>\n",
       "      <th>new_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>TENNESSEE: We're the best state. Nobody even c...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.803353</td>\n",
       "      <td>3.213413</td>\n",
       "      <td>2.013413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A man inserted an advertisement in the classif...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.996359</td>\n",
       "      <td>3.985436</td>\n",
       "      <td>1.885436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>How many men does it take to open a can of bee...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.994951</td>\n",
       "      <td>3.979802</td>\n",
       "      <td>1.579802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Told my mom I hit 1200 Twitter followers. She ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.998251</td>\n",
       "      <td>3.993006</td>\n",
       "      <td>2.993006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Roses are dead. Love is fake. Weddings are bas...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.990869</td>\n",
       "      <td>3.963475</td>\n",
       "      <td>3.863475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>7996</td>\n",
       "      <td>Lack of awareness of the pervasiveness of raci...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.001270</td>\n",
       "      <td>0.005080</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>7997</td>\n",
       "      <td>Why are aspirins white? Because they work sorry</td>\n",
       "      <td>1</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>0.993704</td>\n",
       "      <td>3.974817</td>\n",
       "      <td>0.124817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>7998</td>\n",
       "      <td>Today, we Americans celebrate our independence...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.033051</td>\n",
       "      <td>0.132206</td>\n",
       "      <td>0.132206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>7999</td>\n",
       "      <td>How to keep the flies off the bride at an Ital...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.994729</td>\n",
       "      <td>3.978916</td>\n",
       "      <td>0.978916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>8000</td>\n",
       "      <td>\"Each ounce of sunflower seeds gives you 37% o...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>0.003515</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  is_humor  \\\n",
       "0        1  TENNESSEE: We're the best state. Nobody even c...         1   \n",
       "1        2  A man inserted an advertisement in the classif...         1   \n",
       "2        3  How many men does it take to open a can of bee...         1   \n",
       "3        4  Told my mom I hit 1200 Twitter followers. She ...         1   \n",
       "4        5  Roses are dead. Love is fake. Weddings are bas...         1   \n",
       "...    ...                                                ...       ...   \n",
       "7995  7996  Lack of awareness of the pervasiveness of raci...         0   \n",
       "7996  7997    Why are aspirins white? Because they work sorry         1   \n",
       "7997  7998  Today, we Americans celebrate our independence...         1   \n",
       "7998  7999  How to keep the flies off the bride at an Ital...         1   \n",
       "7999  8000  \"Each ounce of sunflower seeds gives you 37% o...         0   \n",
       "\n",
       "      humor_rating  humor_controversy  offense_rating     score   My_rate  \\\n",
       "0             2.42                1.0            0.20  0.803353  3.213413   \n",
       "1             2.50                1.0            1.10  0.996359  3.985436   \n",
       "2             1.95                0.0            2.40  0.994951  3.979802   \n",
       "3             2.11                1.0            0.00  0.998251  3.993006   \n",
       "4             2.78                0.0            0.10  0.990869  3.963475   \n",
       "...            ...                ...             ...       ...       ...   \n",
       "7995          0.00                0.0            0.25  0.001270  0.005080   \n",
       "7996          1.33                0.0            3.85  0.993704  3.974817   \n",
       "7997          2.55                0.0            0.00  0.033051  0.132206   \n",
       "7998          1.00                0.0            3.00  0.994729  3.978916   \n",
       "7999          0.00                0.0            0.00  0.000879  0.003515   \n",
       "\n",
       "      new_rate  \n",
       "0     2.013413  \n",
       "1     1.885436  \n",
       "2     1.579802  \n",
       "3     2.993006  \n",
       "4     3.863475  \n",
       "...        ...  \n",
       "7995 -0.000000  \n",
       "7996  0.124817  \n",
       "7997  0.132206  \n",
       "7998  0.978916  \n",
       "7999  0.000000  \n",
       "\n",
       "[8000 rows x 9 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "demonstrated-wheel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 60, 64)            1280000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_7 (Spatial (None, 60, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 56, 128)           41088     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 250)               32250     \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 1,353,589\n",
      "Trainable params: 1,353,589\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau \n",
    "from tensorflow.keras.layers import Flatten, Conv2D\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "\n",
    "model_cnn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.SpatialDropout1D(0.2),\n",
    "    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
    "    tf.keras.layers.GlobalMaxPooling1D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(250, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2)\n",
    "model_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "impressed-marriage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 2000 samples\n",
      "Epoch 1/15\n",
      "6000/6000 [==============================] - 12s 2ms/sample - loss: 0.5125 - acc: 0.7325 - val_loss: 0.3470 - val_acc: 0.8495\n",
      "Epoch 2/15\n",
      "6000/6000 [==============================] - 10s 2ms/sample - loss: 0.2322 - acc: 0.9070 - val_loss: 0.3314 - val_acc: 0.8595\n",
      "Epoch 3/15\n",
      "6000/6000 [==============================] - 9s 2ms/sample - loss: 0.0765 - acc: 0.9742 - val_loss: 0.4057 - val_acc: 0.8435\n",
      "Epoch 4/15\n",
      "6000/6000 [==============================] - 10s 2ms/sample - loss: 0.0185 - acc: 0.9950 - val_loss: 0.5161 - val_acc: 0.8450\n",
      "Epoch 5/15\n",
      "6000/6000 [==============================] - 9s 2ms/sample - loss: 0.0053 - acc: 0.9993 - val_loss: 0.5138 - val_acc: 0.8465\n",
      "Epoch 6/15\n",
      "6000/6000 [==============================] - 10s 2ms/sample - loss: 0.0041 - acc: 0.9997 - val_loss: 0.5208 - val_acc: 0.8450\n",
      "Epoch 7/15\n",
      "6000/6000 [==============================] - 9s 2ms/sample - loss: 0.0035 - acc: 0.9998 - val_loss: 0.5216 - val_acc: 0.8460\n",
      "Epoch 8/15\n",
      "6000/6000 [==============================] - 9s 2ms/sample - loss: 0.0035 - acc: 0.9998 - val_loss: 0.5224 - val_acc: 0.8455\n",
      "Epoch 9/15\n",
      "6000/6000 [==============================] - 9s 2ms/sample - loss: 0.0033 - acc: 0.9997 - val_loss: 0.5225 - val_acc: 0.8455\n",
      "Epoch 10/15\n",
      "6000/6000 [==============================] - 9s 2ms/sample - loss: 0.0039 - acc: 0.9997 - val_loss: 0.5226 - val_acc: 0.8455\n",
      "Epoch 11/15\n",
      "6000/6000 [==============================] - 9s 2ms/sample - loss: 0.0035 - acc: 0.9998 - val_loss: 0.5226 - val_acc: 0.8455\n",
      "Epoch 12/15\n",
      "6000/6000 [==============================] - 9s 2ms/sample - loss: 0.0040 - acc: 0.9995 - val_loss: 0.5227 - val_acc: 0.8455\n",
      "Epoch 13/15\n",
      "6000/6000 [==============================] - 9s 2ms/sample - loss: 0.0034 - acc: 0.9998 - val_loss: 0.5227 - val_acc: 0.8455\n",
      "Epoch 14/15\n",
      "6000/6000 [==============================] - 9s 2ms/sample - loss: 0.0037 - acc: 0.9998 - val_loss: 0.5227 - val_acc: 0.8455\n",
      "Epoch 15/15\n",
      "6000/6000 [==============================] - 9s 2ms/sample - loss: 0.0034 - acc: 0.9997 - val_loss: 0.5227 - val_acc: 0.8455\n"
     ]
    }
   ],
   "source": [
    "history_cnn = model_cnn.fit(training_padded, training_labels, batch_size=32, epochs=num_epochs, \n",
    "                    validation_data=(testing_padded, testing_labels), \n",
    "                    callbacks=[rlrp] ,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "regulated-dinner",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE1: 0.6668321638230452\n",
      "Pearson Corr.1 : 0.7482126771279378\n",
      "Spearman corr.1 : 0.7235122194662413\n",
      "kendall.1 : 0.5448200637707373\n",
      "Train on 2000 samples, validate on 6000 samples\n",
      "Epoch 1/30\n",
      " - 3s - loss: 3.1009 - val_loss: 2.4047\n",
      "Epoch 2/30\n",
      " - 0s - loss: 1.8740 - val_loss: 1.2179\n",
      "Epoch 3/30\n",
      " - 0s - loss: 0.7871 - val_loss: 0.4365\n",
      "Epoch 4/30\n",
      " - 0s - loss: 0.4061 - val_loss: 0.3445\n",
      "Epoch 5/30\n",
      " - 0s - loss: 0.3399 - val_loss: 0.2892\n",
      "Epoch 6/30\n",
      " - 0s - loss: 0.2895 - val_loss: 0.2476\n",
      "Epoch 7/30\n",
      " - 0s - loss: 0.2571 - val_loss: 0.2245\n",
      "Epoch 8/30\n",
      " - 0s - loss: 0.2365 - val_loss: 0.2069\n",
      "Epoch 9/30\n",
      " - 0s - loss: 0.2230 - val_loss: 0.1972\n",
      "Epoch 10/30\n",
      " - 0s - loss: 0.2134 - val_loss: 0.1900\n",
      "Epoch 11/30\n",
      " - 0s - loss: 0.2073 - val_loss: 0.1848\n",
      "Epoch 12/30\n",
      " - 0s - loss: 0.2017 - val_loss: 0.1810\n",
      "Epoch 13/30\n",
      " - 0s - loss: 0.1981 - val_loss: 0.1784\n",
      "Epoch 14/30\n",
      " - 0s - loss: 0.1952 - val_loss: 0.1761\n",
      "Epoch 15/30\n",
      " - 0s - loss: 0.1933 - val_loss: 0.1749\n",
      "Epoch 16/30\n",
      " - 0s - loss: 0.1918 - val_loss: 0.1738\n",
      "Epoch 17/30\n",
      " - 0s - loss: 0.1909 - val_loss: 0.1728\n",
      "Epoch 18/30\n",
      " - 0s - loss: 0.1891 - val_loss: 0.1740\n",
      "Epoch 19/30\n",
      " - 0s - loss: 0.1905 - val_loss: 0.1713\n",
      "Epoch 20/30\n",
      " - 0s - loss: 0.1889 - val_loss: 0.1712\n",
      "Epoch 21/30\n",
      " - 0s - loss: 0.1872 - val_loss: 0.1715\n",
      "Epoch 22/30\n",
      " - 0s - loss: 0.1882 - val_loss: 0.1708\n",
      "Epoch 23/30\n",
      " - 0s - loss: 0.1870 - val_loss: 0.1702\n",
      "Epoch 24/30\n",
      " - 0s - loss: 0.1867 - val_loss: 0.1713\n",
      "Epoch 25/30\n",
      " - 0s - loss: 0.1860 - val_loss: 0.1706\n",
      "Epoch 26/30\n",
      " - 0s - loss: 0.1873 - val_loss: 0.1698\n",
      "Epoch 27/30\n",
      " - 0s - loss: 0.1859 - val_loss: 0.1708\n",
      "Epoch 28/30\n",
      " - 0s - loss: 0.1847 - val_loss: 0.1698\n",
      "Epoch 29/30\n",
      " - 0s - loss: 0.1848 - val_loss: 0.1705\n",
      "Epoch 30/30\n",
      " - 0s - loss: 0.1852 - val_loss: 0.1717\n",
      "MAE2: 0.4124679048542678\n",
      "Pearson Corr.2 : 0.8759273979567821\n",
      "Spearman corr.2 : 0.8283389398772314\n",
      "kendall.2 : 0.6494990215888113\n",
      "      original       1st       2nd\n",
      "0         2.42  2.795285  2.305893\n",
      "1         2.50  1.899846  1.772203\n",
      "2         1.95  1.599404  0.757725\n",
      "3         2.11  2.999949  2.417174\n",
      "4         2.78  3.886942  2.179354\n",
      "...        ...       ...       ...\n",
      "7995      0.00 -0.000000 -0.000000\n",
      "7996      1.33  0.147743  0.482892\n",
      "7997      2.55  0.011777  2.189920\n",
      "7998      1.00  0.999993  0.566406\n",
      "7999      0.00  0.000000 -0.000000\n",
      "\n",
      "[8000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "Score_Predict2(df,model_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "alternative-muslim",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>is_humor</th>\n",
       "      <th>humor_rating</th>\n",
       "      <th>humor_controversy</th>\n",
       "      <th>offense_rating</th>\n",
       "      <th>score</th>\n",
       "      <th>My_rate</th>\n",
       "      <th>new_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>TENNESSEE: We're the best state. Nobody even c...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>9.988213e-01</td>\n",
       "      <td>3.995285</td>\n",
       "      <td>2.795285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A man inserted an advertisement in the classif...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>9.999614e-01</td>\n",
       "      <td>3.999846</td>\n",
       "      <td>1.899846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>How many men does it take to open a can of bee...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.40</td>\n",
       "      <td>9.998510e-01</td>\n",
       "      <td>3.999404</td>\n",
       "      <td>1.599404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Told my mom I hit 1200 Twitter followers. She ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.999874e-01</td>\n",
       "      <td>3.999949</td>\n",
       "      <td>2.999949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Roses are dead. Love is fake. Weddings are bas...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9.967354e-01</td>\n",
       "      <td>3.986942</td>\n",
       "      <td>3.886942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>7996</td>\n",
       "      <td>Lack of awareness of the pervasiveness of raci...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.363690e-04</td>\n",
       "      <td>0.000545</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>7997</td>\n",
       "      <td>Why are aspirins white? Because they work sorry</td>\n",
       "      <td>1</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>9.994358e-01</td>\n",
       "      <td>3.997743</td>\n",
       "      <td>0.147743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>7998</td>\n",
       "      <td>Today, we Americans celebrate our independence...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.944199e-03</td>\n",
       "      <td>0.011777</td>\n",
       "      <td>0.011777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>7999</td>\n",
       "      <td>How to keep the flies off the bride at an Ital...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>9.999983e-01</td>\n",
       "      <td>3.999993</td>\n",
       "      <td>0.999993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>8000</td>\n",
       "      <td>\"Each ounce of sunflower seeds gives you 37% o...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.963583e-07</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  is_humor  \\\n",
       "0        1  TENNESSEE: We're the best state. Nobody even c...         1   \n",
       "1        2  A man inserted an advertisement in the classif...         1   \n",
       "2        3  How many men does it take to open a can of bee...         1   \n",
       "3        4  Told my mom I hit 1200 Twitter followers. She ...         1   \n",
       "4        5  Roses are dead. Love is fake. Weddings are bas...         1   \n",
       "...    ...                                                ...       ...   \n",
       "7995  7996  Lack of awareness of the pervasiveness of raci...         0   \n",
       "7996  7997    Why are aspirins white? Because they work sorry         1   \n",
       "7997  7998  Today, we Americans celebrate our independence...         1   \n",
       "7998  7999  How to keep the flies off the bride at an Ital...         1   \n",
       "7999  8000  \"Each ounce of sunflower seeds gives you 37% o...         0   \n",
       "\n",
       "      humor_rating  humor_controversy  offense_rating         score   My_rate  \\\n",
       "0             2.42                1.0            0.20  9.988213e-01  3.995285   \n",
       "1             2.50                1.0            1.10  9.999614e-01  3.999846   \n",
       "2             1.95                0.0            2.40  9.998510e-01  3.999404   \n",
       "3             2.11                1.0            0.00  9.999874e-01  3.999949   \n",
       "4             2.78                0.0            0.10  9.967354e-01  3.986942   \n",
       "...            ...                ...             ...           ...       ...   \n",
       "7995          0.00                0.0            0.25  1.363690e-04  0.000545   \n",
       "7996          1.33                0.0            3.85  9.994358e-01  3.997743   \n",
       "7997          2.55                0.0            0.00  2.944199e-03  0.011777   \n",
       "7998          1.00                0.0            3.00  9.999983e-01  3.999993   \n",
       "7999          0.00                0.0            0.00  7.963583e-07  0.000003   \n",
       "\n",
       "      new_rate  \n",
       "0     2.795285  \n",
       "1     1.899846  \n",
       "2     1.599404  \n",
       "3     2.999949  \n",
       "4     3.886942  \n",
       "...        ...  \n",
       "7995 -0.000000  \n",
       "7996  0.147743  \n",
       "7997  0.011777  \n",
       "7998  0.999993  \n",
       "7999  0.000000  \n",
       "\n",
       "[8000 rows x 9 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "distinguished-differential",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('My_embedding')\n",
    "model_rnn.save('My_rnn')\n",
    "model_lstm_avg.save('My_lstm_avg')\n",
    "model_lstm1.save('My_lstm_max')\n",
    "model_cnn.save('My_cnn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "biblical-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "found-carol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DELL\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 60, 64)            1280000   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 60, 64)            24832     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,305,889\n",
      "Trainable params: 1,305,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau \n",
    "\n",
    "model_bidir = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
    "    tf.keras.layers.GlobalMaxPool1D(),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2)\n",
    "model_bidir.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_bidir.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "major-idaho",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 2000 samples\n",
      "WARNING:tensorflow:From C:\\Users\\DELL\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/5\n",
      "6000/6000 [==============================] - 26s 4ms/sample - loss: 0.5421 - acc: 0.7090 - val_loss: 0.3773 - val_acc: 0.8445\n",
      "Epoch 2/5\n",
      "6000/6000 [==============================] - 22s 4ms/sample - loss: 0.2571 - acc: 0.9012 - val_loss: 0.3228 - val_acc: 0.8725\n",
      "Epoch 3/5\n",
      "6000/6000 [==============================] - 22s 4ms/sample - loss: 0.1091 - acc: 0.9643 - val_loss: 0.4298 - val_acc: 0.8545\n",
      "Epoch 4/5\n",
      "6000/6000 [==============================] - 22s 4ms/sample - loss: 0.0466 - acc: 0.9862 - val_loss: 0.5111 - val_acc: 0.8510 lo\n",
      "Epoch 5/5\n",
      "6000/6000 [==============================] - 22s 4ms/sample - loss: 0.0195 - acc: 0.9957 - val_loss: 0.5288 - val_acc: 0.8535\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training the model\n",
    "num_epochs = 5\n",
    "\n",
    "history_bidir = model_bidir.fit(training_padded, training_labels, batch_size=32, epochs=num_epochs, \n",
    "                    validation_data=(testing_padded, testing_labels), \n",
    "                    callbacks=[rlrp] ,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "temporal-teacher",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE1: 0.6345295145344734\n",
      "Pearson Corr.1 : 0.7849685955260489\n",
      "Spearman corr.1 : 0.7511649244170083\n",
      "kendall.1 : 0.5713070767073772\n",
      "Train on 2000 samples, validate on 6000 samples\n",
      "Epoch 1/5\n",
      " - 1s - loss: 2.7837 - val_loss: 2.2400\n",
      "Epoch 2/5\n",
      " - 0s - loss: 1.7845 - val_loss: 1.2781\n",
      "Epoch 3/5\n",
      " - 0s - loss: 1.0513 - val_loss: 0.8457\n",
      "Epoch 4/5\n",
      " - 0s - loss: 0.7848 - val_loss: 0.6914\n",
      "Epoch 5/5\n",
      " - 0s - loss: 0.6600 - val_loss: 0.6074\n",
      "MAE2: 0.6835966731414198\n",
      "Pearson Corr.2 : 0.8023747512708381\n",
      "Spearman corr.2 : 0.760306070857525\n",
      "kendall.2 : 0.5503121402659081\n"
     ]
    }
   ],
   "source": [
    "Score_Predict2(df,model_bidir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "efficient-television",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import scipy.stats\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) \n",
    "def Score_Predict2(df,model):\n",
    "    df['score']=df['text'].apply(lambda text: model.predict(pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=max_length, padding=padding_type, truncating=trunc_type)))\n",
    "    df['My_rate']=df['score'].apply(lambda s: s*4)\n",
    "  #  df['My_rate']=df['is_humor'].apply(lambda s: [[0]] if s==0 else None)\n",
    "    df['new_rate'] = df['My_rate'][0][0] - df['humor_controversy']-df['offense_rating']\n",
    "    df.new_rate.abs()\n",
    "    df['humor_rating'] = df['humor_rating'].replace(np.nan, 0)\n",
    "    df['new_rate'] = df['new_rate'].replace(np.nan, 0)\n",
    "    df['new_rate'] = df['new_rate']*df['is_humor']\n",
    "    actual = []\n",
    "    pred = []   \n",
    "    for i,l,text,humour,rating,controversy,offense,score,my_rate,new_rate in df.itertuples():\n",
    "            actual.append(rating)\n",
    "            pred.append(new_rate)\n",
    "    mae = metrics.mean_absolute_error(actual, pred)\n",
    "    print(\"MAE1:\",mae)\n",
    "    print(\"Pearson Corr.1 :\",scipy.stats.pearsonr(actual, pred)[0] )\n",
    "    print(\"Spearman corr.1 :\",scipy.stats.spearmanr(actual, pred)[0])\n",
    "    print(\"kendall.1 :\",scipy.stats.kendalltau(actual, pred)[0])\n",
    "    #2nd method\n",
    "    features = df[['humor_controversy','offense_rating','score']].values\n",
    "    labels = df['humor_rating'].values\n",
    "    # getting training and testing data\n",
    "    features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size = .75, random_state = 0)\n",
    "    df['humor_controversy'] = df['humor_controversy'].replace(np.nan, 0)\n",
    "    scaler=MinMaxScaler()\n",
    "    scaler.fit(features_train)\n",
    "    features_train=scaler.transform(features_train)\n",
    "    #   print(features_train)\n",
    "    features_test=scaler.transform(features_test)\n",
    "    #print(features_test)\n",
    "    model1=Sequential()\n",
    "    model1.add(Dense(4,activation='relu'))\n",
    "    model1.add(Dense(4,activation='relu'))\n",
    "    model1.add(Dense(4,activation='relu'))\n",
    "    model1.add(Dense(1))\n",
    "    model1.compile(loss='mse',optimizer='adam')#,metrics=['accuracy'])\n",
    "    history=model1.fit(x=features_train,y=labels_train,epochs=5,validation_data=(features_test, labels_test),verbose=2)\n",
    "    #humor_pred=model1.predict(features_test)\n",
    "    pred1 = []    \n",
    "    for i,l,text,humour,rating,controversy,offense,score,my_rate,new_rate in df.itertuples():\n",
    "            new=[[controversy,offense,score]]\n",
    "            new=scaler.transform(new)\n",
    "            pred1.append(model1.predict(new)[0][0])\n",
    "    #print(pred1)        \n",
    "    mae2 = metrics.mean_absolute_error(actual, pred1)\n",
    "    print(\"MAE2:\",mae2)\n",
    "    print(\"Pearson Corr.2 :\",scipy.stats.pearsonr(actual, pred1)[0] )\n",
    "    print(\"Spearman corr.2 :\",scipy.stats.spearmanr(actual, pred1)[0])\n",
    "    print(\"kendall.2 :\",scipy.stats.kendalltau(actual, pred1)[0])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "indian-dairy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 60, 64)            1280000   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 60, 32)            9312      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,289,857\n",
      "Trainable params: 1,289,857\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_gru = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.GRU(32, dropout=0.2, recurrent_dropout=0.2, return_sequences=True),\n",
    "    tf.keras.layers.GlobalMaxPooling1D(),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2)\n",
    "model_gru.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_gru.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ignored-peace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 2000 samples\n",
      "Epoch 1/30\n",
      "6000/6000 [==============================] - 15s 3ms/sample - loss: 0.0418 - acc: 0.9900 - val_loss: 0.4848 - val_acc: 0.8405\n",
      "Epoch 2/30\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0374 - acc: 0.9913 - val_loss: 0.5190 - val_acc: 0.8370\n",
      "Epoch 3/30\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0343 - acc: 0.9922 - val_loss: 0.5131 - val_acc: 0.8440\n",
      "Epoch 4/30\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0307 - acc: 0.9933 - val_loss: 0.5154 - val_acc: 0.8430\n",
      "Epoch 5/30\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0312 - acc: 0.9943 - val_loss: 0.5181 - val_acc: 0.8430\n",
      "Epoch 6/30\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0307 - acc: 0.9933 - val_loss: 0.5183 - val_acc: 0.8430\n",
      "Epoch 7/30\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0301 - acc: 0.9940 - val_loss: 0.5186 - val_acc: 0.8430\n",
      "Epoch 8/30\n",
      "6000/6000 [==============================] - 15s 2ms/sample - loss: 0.0271 - acc: 0.9948 - val_loss: 0.5186 - val_acc: 0.8430\n",
      "Epoch 9/30\n",
      "6000/6000 [==============================] - 15s 2ms/sample - loss: 0.0306 - acc: 0.9923 - val_loss: 0.5186 - val_acc: 0.8430\n",
      "Epoch 10/30\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0285 - acc: 0.9940 - val_loss: 0.5186 - val_acc: 0.8430\n",
      "Epoch 11/30\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0311 - acc: 0.9933 - val_loss: 0.5186 - val_acc: 0.8430\n",
      "Epoch 12/30\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0315 - acc: 0.9935 - val_loss: 0.5186 - val_acc: 0.8430\n",
      "Epoch 13/30\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0309 - acc: 0.9937 - val_loss: 0.5186 - val_acc: 0.8430\n",
      "Epoch 14/30\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0292 - acc: 0.9940 - val_loss: 0.5186 - val_acc: 0.8430\n",
      "Epoch 15/30\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0293 - acc: 0.9937 - val_loss: 0.5186 - val_acc: 0.8430\n",
      "Epoch 16/30\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0310 - acc: 0.9928 - val_loss: 0.5186 - val_acc: 0.8430\n",
      "Epoch 17/30\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0298 - acc: 0.9940 - val_loss: 0.5186 - val_acc: 0.8430\n",
      "Epoch 18/30\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0293 - acc: 0.9935 - val_loss: 0.5186 - val_acc: 0.8430\n",
      "Epoch 19/30\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0312 - acc: 0.9942 - val_loss: 0.5186 - val_acc: 0.8430\n",
      "Epoch 20/30\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0298 - acc: 0.9950 - val_loss: 0.5186 - val_acc: 0.8430\n",
      "Epoch 21/30\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0291 - acc: 0.9947 - val_loss: 0.5186 - val_acc: 0.8430\n",
      "Epoch 22/30\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0313 - acc: 0.9937 - val_loss: 0.5186 - val_acc: 0.8430\n",
      "Epoch 23/30\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0307 - acc: 0.9932 - val_loss: 0.5186 - val_acc: 0.8430\n",
      "Epoch 24/30\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0307 - acc: 0.9935 - val_loss: 0.5186 - val_acc: 0.8430\n",
      "Epoch 25/30\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0306 - acc: 0.9938 - val_loss: 0.5186 - val_acc: 0.8430\n",
      "Epoch 26/30\n",
      "6000/6000 [==============================] - 15s 2ms/sample - loss: 0.0294 - acc: 0.9942 - val_loss: 0.5186 - val_acc: 0.8430\n",
      "Epoch 27/30\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0293 - acc: 0.9937 - val_loss: 0.5186 - val_acc: 0.8430\n",
      "Epoch 28/30\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0311 - acc: 0.9938 - val_loss: 0.5186 - val_acc: 0.8430\n",
      "Epoch 29/30\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0289 - acc: 0.9945 - val_loss: 0.5186 - val_acc: 0.8430\n",
      "Epoch 30/30\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0286 - acc: 0.9945 - val_loss: 0.5186 - val_acc: 0.8430\n"
     ]
    }
   ],
   "source": [
    "history_gru = model_gru.fit(training_padded, training_labels, batch_size=32, epochs=5, \n",
    "                    validation_data=(testing_padded, testing_labels), \n",
    "                    callbacks=[rlrp] ,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "combined-hunger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE1: 0.637353477563858\n",
      "Pearson Corr.1 : 0.786476922451849\n",
      "Spearman corr.1 : 0.7511649244170083\n",
      "kendall.1 : 0.5713070767073772\n",
      "Train on 2000 samples, validate on 6000 samples\n",
      "Epoch 1/5\n",
      " - 2s - loss: 2.9844 - val_loss: 2.5435\n",
      "Epoch 2/5\n",
      " - 0s - loss: 1.8582 - val_loss: 1.1701\n",
      "Epoch 3/5\n",
      " - 0s - loss: 0.9035 - val_loss: 0.7494\n",
      "Epoch 4/5\n",
      " - 0s - loss: 0.7217 - val_loss: 0.6664\n",
      "Epoch 5/5\n",
      " - 0s - loss: 0.6437 - val_loss: 0.5932\n",
      "MAE2: 0.673437413840592\n",
      "Pearson Corr.2 : 0.8330928855402568\n",
      "Spearman corr.2 : 0.7635412109353715\n",
      "kendall.2 : 0.5536798693927716\n"
     ]
    }
   ],
   "source": [
    "Score_Predict2(df,model_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-handy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
